# -*- coding: utf-8 -*-
"""[JThiongo] Data pipelines with Python and MongoDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f6LwhihfpA9bDxRe9PmorEaxZM9sn7OE

#Telecommunications Fraud Detection Using MongoDB and Python

###Background Information

Telecommunications companies generate a vast amount of data daily, which can be used to
detect fraud. Fraudulent activity can lead to substantial financial losses and damage the
company's reputation. With the help of data pipelines, companies can detect fraud before it
escalates.

##Problem Statement

Telecommunications companies need to detect fraudulent activities such as unauthorized use of
premium services or fake billing. Building a data pipeline with MongoDB and Python could help
identify suspicious activity by extracting data from billing systems, call logs, and other sources,
transforming the data to identify patterns or anomalies, and storing it in MongoDB for further
analysis

###The Solution
"""

pip install pymongo

#Import libraries needed

import pandas as pd
import pymongo
import logging
import zlib
from pprint import pprint
import pymongo.bulk
import csv

"""####Extraction ...."""

# Extraction function
def extract_data(csv1, csv2):

  """
    Use the pandas library to read the input CSV files.
	  return 2 dfs
  """

  try:
    # Load call log data from CSV file
    df_calls = pd.read_csv(csv1)

    # Load billing data from CSV file
    df_billing = pd.read_csv(csv2)

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Data extraction completed.")

  except Exception as e:
    print(e)

  return df_calls, df_billing

df_calls, df_billing = extract_data('call_logs.csv','billing_systems.csv')

df_billing.dtypes

df_calls.dtypes

##Explore nulls
df_billing.isnull().sum()

df_calls.isnull().sum()

"""####Transformation ...."""

# Data cleaning and handling missing values, 

def transformData(df_calls, df_billing):
  """
    Function takes 2dfs as arguments
    Performs data conversion, enriches data by adding duration_minutes column
  """

  try:
    ##df_calls
    ##Add call_duration in minutes column || convert call_type to category || convert call_date to datetime

    df_calls['duration_minutes'] = df_calls['call_duration'] / 60

    df_calls['call_type'] = df_calls['call_type'].astype("category")

    df_calls['call_date'] = pd.to_datetime(df_calls['call_date'])

    ##df_billing
    ##Convert transaction type to category || convert transaction date to datetime

    df_billing['transaction_type'] = df_billing['transaction_type'].astype("category")

    df_billing['transaction_date'] = pd.to_datetime(df_billing['transaction_date'])

  except Exception as e:
    print(e)

  return df_calls, df_billing

df_calls, df_billing = transformData(df_calls, df_billing)

#Exploring datasets descriptive statistics
##Call logs
calls1 = df_calls.groupby('call_type')['caller_number'].count()
calls1

#Average call duration
calls2 = df_calls.groupby('call_type')['duration_minutes'].mean()
calls2

#Average transaction spend
bills1 = df_billing.groupby('transaction_type')['transaction_amount'].mean()
bills1

#Average spend per customer...
bills2   = df_billing.groupby('customer_id')['transaction_amount'].mean()
bills2

#Compression function
# 
def compress_data(data):

  """
  takes a df as argument. Returns compressed df as result
  """
  df_bytes = data.to_json().encode()
  # Compress data using zlib
  compressed_data = zlib.compress(df_bytes)
  return compressed_data

"""####Loading ....

"""

##Load function 1 - Create Index, Load DF no compression [using call_logs.csv]

def loadNoCompress(data):

  """
  Function to load data to mongodb without any compression. Takes df as argument 
  """

  try:
    # Connect to MongoDB
    #connection string with connection pooling option
    conn = "mongodb+srv://kmaild2010:Siri2022@cluster0.pbxtwyb.mongodb.net/myFirstDatabase?maxPoolSize=50"

    #Enabling ssl option and write concern acknowledgments
    client = pymongo.MongoClient(conn,ssl=True, w=1)

    # Select database and collection
    db = client["call_db"]
    collection = db["no_compress"]
    
    #Build index
    collection.create_index([('call_id', 1)])

    # Insert the transformed data into the MongoDB database
    result = collection.insert_many(data.to_dict('records'))

    # Close the database connection
    client.close()
  
  except Exception as e:
    print(e)
  
  # Use Python logging module to log errors and activities
  logger = logging.getLogger(__name__)
  logger.info("load no compression - completed")
  
  return print("Below the inserted items: \n",result.inserted_ids)

###Load function 2 - Load compressed data [using call_logs.csv]

def loadCompress(compressed_data):

  """
  Function to load compressed data to mongodb. Takes compressed df as argument
  """

  try:
    # Connect to MongoDB
    #connection string with connection pooling option
    conn = "mongodb+srv://kmaild2010:Siri2022@cluster0.pbxtwyb.mongodb.net/myFirstDatabase?maxPoolSize=50"

    #Enabling ssl option
    client = pymongo.MongoClient(conn,ssl=True)

    # Select database and collection
    db = client["call_db"]
    collection = db["with_compression"]
    

    # Insert compressed data into MongoDB collection
    collection.insert_one({'data': compressed_data})

    # Get the database performance stats
    stats = db.command("dbStats")
    # Print the stats
    print("Output of dbStats \n:",stats)

  
    # Close the database connection
    client.close()

  except Exception as e:
    print(e)
  
  # Use Python logging module to log errors and activities
  logger = logging.getLogger(__name__)
  logger.info("load with compression - completed")

###Load function 3 -  Bulk insert  [using billing_system csv]
##!Both csv files have less than 10 records - bulk insert function not needed 

def loadBulkInsert(data):

  """
  Function to perform bulk insert into mongodb. Takes df as argument 
  """

  try:
    # Connect to MongoDB
    #connection string with connection pooling option
    conn = "mongodb+srv://kmaild2010:Siri2022@cluster0.pbxtwyb.mongodb.net/myFirstDatabase?maxPoolSize=50"

    #Enabling ssl option
    client = pymongo.MongoClient(conn,ssl=True)

    # Select database and collection
    db = client["call_db"]
    collection = db["bulk_insert"]

    bulk_operations = []
    for doc in data.to_dict('records'):
        bulk_operations.append(collection.update_one({'_id': doc['transaction_id']}, {'$set': doc}, upsert=True))
    collection.bulk_write(bulk_operations)

    # Close the database connection
    client.close()

  except Exception as e:
    print(e)
  # Use Python logging module to log errors and activities
  logger = logging.getLogger(__name__)
  logger.info("load bulk insert - completed")

###Load function 4 - aggregation pipeline [using call_logs.csv]

def loadAggregate():

  """
  Function to perform data aggregation on contents of mongodb collection
  """

  try:
    # Connect to MongoDB
    #connection string with connection pooling option
    conn = "mongodb+srv://kmaild2010:Siri2022@cluster0.pbxtwyb.mongodb.net/myFirstDatabase?maxPoolSize=50"

    #Enabling ssl option
    client = pymongo.MongoClient(conn,ssl=True)

    # Select database and collection
    db = client["call_db"]
    collection = db["no_compress"]

    pipeline = [
          {
              '$group': {
                  '_id': '$caller_number',
                  'total_minutes': {'$sum': '$duration_minutes'},
                  'count': {'$sum': 1}
              }
          },
          {
              '$sort': {'total_minutes': -1}
          }
      ]
    result = collection.aggregate(pipeline)
    for doc in result:
      print(doc)
    
  except Exception as e:
    print(e)
  # Close the database connection
  client.close()

  # Use Python logging module to log errors and activities
  logger = logging.getLogger(__name__)
  logger.info("load aggregate1 - completed")

"""####Pipelines...."""

#Pipeline  load1 nocompress

def main_1():

  """
  Option 1 of pipeline automation
  """
  #extract  
  df_calls, df_billing = extract_data('call_logs.csv','billing_systems.csv')
  #transform
  df_calls, df_billing = transformData(df_calls, df_billing)
  #load
  loadNoCompress(df_calls)

#Pipeline load2 with compress

def main_2():

  """
  Option 2 of pipeline automation
  """
  #extract
  df_calls, df_billing = extract_data('call_logs.csv','billing_systems.csv')
  #transform
  df_calls, df_billing = transformData(df_calls, df_billing)
  #load
  loadCompress(compress_data(df_calls))

#Pipeline load4 with aggregation 

def main_3():

  """
  Option 3 of pipeline automation
  """
  #Do aggregation and print
  loadAggregate()

##Main function

option_str="""Select an option below [1,2,3]\
  Option 1 - ETL with no data compression\n
  Option 2 - ETL with data compression\n
  Option 3 - Perform data aggregation function\n

"""

if __name__ == '__main__':
  #Get user choice
  option = input(option_str)

  if option == '1':
    main_1()
  elif option == '2':
    main_2()
  elif option == '3':
    main_3()
  else:
    print("Please choose either option 1, 2 or 3 ...")
    option = input(option_str)